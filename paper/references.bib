@article{Hein2017,
    title = "Particle swarm optimization for generating interpretable fuzzy reinforcement learning policies",
    journal = "Engineering Applications of Artificial Intelligence",
    volume = "65",
    pages = "87 - 98",
    year = "2017",
    issn = "0952-1976",
    author = "Daniel Hein and Alexander Hentschel and Thomas Runkler and Steffen Udluft",
    keywords = "Interpretable, Reinforcement learning, Fuzzy policy, Fuzzy controller, Particle swarm optimization",
    doi = {10.1016/j.engappai.2017.07.005},
    abstract = "Fuzzy controllers are efficient and interpretable system controllers for continuous state and action spaces. To date, such controllers have been constructed manually or trained automatically either using expert-generated problem-specific cost functions or incorporating detailed knowledge about the optimal control strategy. Both requirements for automatic training processes are not found in most real-world reinforcement learning (RL) problems. In such applications, online learning is often prohibited for safety reasons because it requires exploration of the problem’s dynamics during policy training. We introduce a fuzzy particle swarm reinforcement learning (FPSRL) approach that can construct fuzzy RL policies solely by training parameters on world models that simulate real system dynamics. These world models are created by employing an autonomous machine learning technique that uses previously generated transition samples of a real system. To the best of our knowledge, this approach is the first to relate self-organizing fuzzy controllers to model-based batch RL. FPSRL is intended to solve problems in domains where online learning is prohibited, system dynamics are relatively easy to model from previously generated default policy transition samples, and it is expected that a relatively easily interpretable control policy exists. The efficiency of the proposed approach with problems from such domains is demonstrated using three standard RL benchmarks, i.e., mountain car, cart-pole balancing, and cart-pole swing-up. Our experimental results demonstrate high-performing, interpretable fuzzy policies."
}

@article{Hein2017a,
    author    = {Daniel Hein and
               Stefan Depeweg and
               Michel Tokic and
               Steffen Udluft and
               Alexander Hentschel and
               Thomas A. Runkler and
               Volkmar Sterzing},
    title     = {A Benchmark Environment Motivated by Industrial Control Problems},
    journal   = {CoRR},
    volume    = {abs/1709.09480},
    year      = {2017},
    archivePrefix = {arXiv},
    eprint    = {1709.09480},
    timestamp = {Tue, 04 Dec 2018 07:36:12 +0100},
    biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1709-09480},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    doi = {10.1109/SSCI.2017.8280935},
}

@article{Hein2018,
    title = "Interpretable policies for reinforcement learning by genetic programming",
    journal = "Engineering Applications of Artificial Intelligence",
    volume = "76",
    pages = "158 - 169",
    year = "2018",
    issn = "0952-1976",
    author = "Daniel Hein and Steffen Udluft and Thomas A. Runkler",
    keywords = "Interpretable, Reinforcement learning, Genetic programming, Model-based, Symbolic regression, Industrial benchmark",
    doi = {10.1016/j.engappai.2018.09.007},
    abstract = "The search for interpretable reinforcement learning policies is of high academic and industrial interest. Especially for industrial systems, domain experts are more likely to deploy autonomously learned controllers if they are understandable and convenient to evaluate. Basic algebraic equations are supposed to meet these requirements, as long as they are restricted to an adequate complexity. Here we introduce the genetic programming for reinforcement learning (GPRL) approach based on model-based batch reinforcement learning and genetic programming, which autonomously learns policy equations from pre-existing default state–action trajectory samples. GPRL is compared to a straightforward method which utilizes genetic programming for symbolic regression, yielding policies imitating an existing well-performing, but non-interpretable policy. Experiments on three reinforcement learning benchmarks, i.e., mountain car, cart–pole balancing, and industrial benchmark, demonstrate the superiority of our GPRL approach compared to the symbolic regression method. GPRL is capable of producing well-performing interpretable reinforcement learning policies from pre-existing default trajectory data."
}

@Inbook{Duell2012,
author="Duell, Siegmund
and Udluft, Steffen
and Sterzing, Volkmar",
editor="Montavon, Grégoire
    and Orr, Geneviève B.
    and Müler, Klaus-Robert",
    title="Solving Partially Observable Reinforcement Learning Problems with Recurrent Neural Networks",
    bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
    year="2012",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="709--733",
    abstract="The aim of this chapter is to provide a series of tricks and recipes for neural state estimation, particularly for real world applications of reinforcement learning. We use various topologies of recurrent neural networks as they allow to identify the continuous valued, possibly high dimensional state space of complex dynamical systems. Recurrent neural networks explicitly offer possibilities to account for time and memory, in principle they are able to model any type of dynamical system. Because of these capabilities recurrent neural networks are a suitable tool to approximate a Markovian state space of dynamical systems. In a second step, reinforcement learning methods can be applied to solve a defined control problem. Besides the trick of using a recurrent neural network for state estimation, various issues regarding real world problems such as, large sets of observables and long-term dependencies are addressed.",
    isbn="978-3-642-35289-8",
}

@article{Hein2017b,
    author    = {Daniel Hein and
                Steffen Udluft and
                Michel Tokic and
                Alexander Hentschel and
                Thomas A. Runkler and
                Volkmar Sterzing},
    title     = {Batch Reinforcement Learning on the Industrial Benchmark: First Experiences},
    journal   = {CoRR},
    volume    = {abs/1705.07262},
    year      = {2017},
    archivePrefix = {arXiv},
    eprint    = {1705.07262},
    timestamp = {Tue, 04 Dec 2018 07:36:12 +0100},
    bibsource = {dblp computer science bibliography, https://dblp.org},
    doi = {10.1109/IJCNN.2017.7966389},
}

@article{Miranda2018,
    author  = {Lester James V. Miranda},
    title   = "{P}y{S}warms, a research-toolkit for {P}article {S}warm {O}ptimization in {P}ython",
    journal = {Journal of Open Source Software},
    year    = {2018},
    volume  = {3},
    issue   = {21},
    doi     = {10.21105/joss.00433},
}
